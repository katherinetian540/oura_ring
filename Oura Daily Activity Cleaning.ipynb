{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oura Ring Daily Activity Data Cleaning\n",
    "### Daily Summary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to extract, including contributors dictionary fields\n",
    "columns = [\n",
    "    \"id\", \"day\", \"score\", \"active_calories\", \"average_met_minutes\",\n",
    "    \"high_activity_time\", \"medium_activity_time\", \"low_activity_time\",\n",
    "    \"high_activity_met_minutes\", \"medium_activity_met_minutes\", \"low_activity_met_minutes\",\n",
    "    \"inactivity_alerts\", \"steps\", \"target_calories\", \"meters_to_target\",\n",
    "    \"total_calories\", \"equivalent_walking_distance\", \"non_wear_time\", \"resting_time\",\n",
    "    \"sedentary_met_minutes\", \"sedentary_time\",\n",
    "    \"meet_daily_targets\", \"move_every_hour\", \"recovery_time\", \n",
    "    \"stay_active\", \"training_frequency\", \"training_volume\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of users\n",
    "user_ids_UL = [\n",
    "    'UL_0201', 'UL_0225', 'UL_0352', 'UL_0417', 'UL_0422',\n",
    "    'UL_0480', 'UL_1086', 'UL_1184', 'UL_9900'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All user data combined and saved: /Users/katherinetian/Downloads/Daily Activity/UL/daily_activity_UL.csv\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "directory_UL = '/Users/katherinetian/Downloads/Daily Activity/UL/'\n",
    "\n",
    "# Initialize an empty list to hold all records from all users\n",
    "all_records = []\n",
    "\n",
    "# Process each SA user ID\n",
    "for user_id in user_ids_UL:\n",
    "    file_name = f\"daily_activity_{user_id}.json\"\n",
    "    file_path = os.path.join(directory_UL, file_name)\n",
    "    \n",
    "    # Try to open and process each file\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract relevant data, including contributors fields\n",
    "        for entry in data.get(\"data\", []):\n",
    "            # Basic fields with user_id as the first column\n",
    "            record = {\"user_id\": user_id}\n",
    "            record.update({field: entry.get(field, None) for field in columns if field not in [\"user_id\", \"meet_daily_targets\", \"move_every_hour\", \"recovery_time\", \"stay_active\", \"training_frequency\", \"training_volume\"]})\n",
    "            # Contributors fields\n",
    "            contributors = entry.get(\"contributors\", {})\n",
    "            record.update({\n",
    "                \"meet_daily_targets\": contributors.get(\"meet_daily_targets\", None),\n",
    "                \"move_every_hour\": contributors.get(\"move_every_hour\", None),\n",
    "                \"recovery_time\": contributors.get(\"recovery_time\", None),\n",
    "                \"stay_active\": contributors.get(\"stay_active\", None),\n",
    "                \"training_frequency\": contributors.get(\"training_frequency\", None),\n",
    "                \"training_volume\": contributors.get(\"training_volume\", None)\n",
    "            })\n",
    "            all_records.append(record)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from all records and save as a single CSV file\n",
    "combined_df = pd.DataFrame(all_records, columns=columns)\n",
    "combined_csv_path = os.path.join(directory_UL, \"daily_activity_UL.csv\")\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "print(f\"All user data combined and saved: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 5-minute activity data saved: /Users/katherinetian/Downloads/Daily Activity/UL/activity_intraday_UL.csv\n"
     ]
    }
   ],
   "source": [
    "# Columns for the class_5_min table\n",
    "columns = [\"user_id\", \"day\", \"class_5_min\"]\n",
    "\n",
    "# Initialize an empty list to hold all records for class_5_min\n",
    "class_5_min_records = []\n",
    "\n",
    "# Process each UL user ID\n",
    "for user_id in user_ids_UL:\n",
    "    file_name = f\"daily_activity_{user_id}.json\"\n",
    "    file_path = os.path.join(directory_UL, file_name)\n",
    "    \n",
    "    # Try to open and process each file\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract the class_5_min data and day (date) for each entry\n",
    "        for entry in data.get(\"data\", []):\n",
    "            day = entry.get(\"day\", None)  # Date in the format YYYY-MM-DD\n",
    "            class_5_min = entry.get(\"class_5_min\", None)  # Five-minute interval activity classifications\n",
    "            \n",
    "            if day and class_5_min:\n",
    "                # Create a record for each 5-minute interval\n",
    "                for activity_class in class_5_min:\n",
    "                    # Append each interval as a record\n",
    "                    record = {\n",
    "                        \"user_id\": user_id,\n",
    "                        \"day\": day,\n",
    "                        \"class_5_min\": int(activity_class) if activity_class.isdigit() else None\n",
    "                    }\n",
    "                    class_5_min_records.append(record)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the records and save as a CSV file\n",
    "class_5_min_df = pd.DataFrame(class_5_min_records, columns=columns)\n",
    "class_5_min_csv_path = os.path.join(directory_UL, \"activity_intraday_UL.csv\")\n",
    "class_5_min_df.to_csv(class_5_min_csv_path, index=False)\n",
    "print(f\"Class 5-minute activity data saved: {class_5_min_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MET intraday data combined and saved: /Users/katherinetian/Downloads/Daily Activity/UL/met_intraday_activity_UL.csv\n"
     ]
    }
   ],
   "source": [
    "# Columns for the met_intraday table\n",
    "met_intraday_columns = [\"user_id\", \"datetime\", \"met_intraday\"]\n",
    "\n",
    "# Initialize an empty list to hold all records for met_intraday\n",
    "met_intraday_records = []\n",
    "\n",
    "# Process each UL user ID\n",
    "for user_id in user_ids_UL:\n",
    "    file_name = f\"daily_activity_{user_id}.json\"\n",
    "    file_path = os.path.join(directory_UL, file_name)\n",
    "    \n",
    "    # Try to open and process each file\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract MET data for each entry\n",
    "        for entry in data.get(\"data\", []):\n",
    "            met_data = entry.get(\"met\", {})  # MET data dictionary\n",
    "            \n",
    "            # Extract relevant fields from MET data\n",
    "            interval = met_data.get(\"interval\", None)  # Time interval in seconds between each MET measurement\n",
    "            start_timestamp = met_data.get(\"timestamp\", None)  # Start timestamp for the MET data sequence\n",
    "            items = met_data.get(\"items\", [])  # Array of MET values\n",
    "            \n",
    "            # Check if required MET data is present\n",
    "            if start_timestamp and interval and items:\n",
    "                # Convert the start timestamp to a datetime object\n",
    "                try:\n",
    "                    start_datetime = pd.to_datetime(start_timestamp)\n",
    "                    \n",
    "                    # Create a record for each MET value\n",
    "                    for i, met_value in enumerate(items):\n",
    "                        # Calculate the timestamp for each measurement based on the interval\n",
    "                        datetime_val = start_datetime + pd.Timedelta(seconds=interval * i)\n",
    "                        \n",
    "                        # Append the record to met_intraday_records\n",
    "                        record = {\n",
    "                            \"user_id\": user_id,\n",
    "                            \"datetime\": datetime_val,\n",
    "                            \"met_intraday\": met_value\n",
    "                        }\n",
    "                        met_intraday_records.append(record)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing MET timestamp for participant {user_id}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the met_intraday records\n",
    "met_intraday_df = pd.DataFrame(met_intraday_records, columns=met_intraday_columns)\n",
    "\n",
    "# Save to a CSV file\n",
    "met_intraday_csv_path = os.path.join(directory_UL, \"met_intraday_activity_UL.csv\")\n",
    "met_intraday_df.to_csv(met_intraday_csv_path, index=False)\n",
    "print(f\"MET intraday data combined and saved: {met_intraday_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of SA user IDs\n",
    "user_ids_SA = [\n",
    "    'SA_0071', 'SA_0148', 'SA_0243', 'SA_0528', 'SA_0585','SA_0721', \n",
    "    'SA_0762', 'SA_0811', 'SA_0820', 'SA_1173', 'SA_1207', 'SA_1368'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All user data combined and saved: /Users/katherinetian/Downloads/Daily Activity/SA/daily_activity_SA.csv\n"
     ]
    }
   ],
   "source": [
    "# Directory path\n",
    "directory_SA = '/Users/katherinetian/Downloads/Daily Activity/SA/'\n",
    "\n",
    "# Initialize an empty list to hold all records from all users\n",
    "all_records = []\n",
    "\n",
    "# Process each SA user ID\n",
    "for user_id in user_ids_SA:\n",
    "    file_name = f\"daily_activity_{user_id}.json\"\n",
    "    file_path = os.path.join(directory_SA, file_name)\n",
    "    \n",
    "    # Try to open and process each file\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract relevant data, including contributors fields\n",
    "        for entry in data.get(\"data\", []):\n",
    "            # Basic fields with user_id as the first column\n",
    "            record = {\"user_id\": user_id}\n",
    "            record.update({field: entry.get(field, None) for field in columns if field not in [\"user_id\", \"meet_daily_targets\", \"move_every_hour\", \"recovery_time\", \"stay_active\", \"training_frequency\", \"training_volume\"]})\n",
    "            # Contributors fields\n",
    "            contributors = entry.get(\"contributors\", {})\n",
    "            record.update({\n",
    "                \"meet_daily_targets\": contributors.get(\"meet_daily_targets\", None),\n",
    "                \"move_every_hour\": contributors.get(\"move_every_hour\", None),\n",
    "                \"recovery_time\": contributors.get(\"recovery_time\", None),\n",
    "                \"stay_active\": contributors.get(\"stay_active\", None),\n",
    "                \"training_frequency\": contributors.get(\"training_frequency\", None),\n",
    "                \"training_volume\": contributors.get(\"training_volume\", None)\n",
    "            })\n",
    "            all_records.append(record)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from all records and save as a single CSV file\n",
    "combined_df = pd.DataFrame(all_records, columns=columns)\n",
    "combined_csv_path = os.path.join(directory_SA, \"daily_activity_SA.csv\")\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "print(f\"All user data combined and saved: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 5-minute activity data saved: /Users/katherinetian/Downloads/Daily Activity/SA/activity_intraday_SA.csv\n"
     ]
    }
   ],
   "source": [
    "# Columns for the class_5_min table\n",
    "columns = [\"user_id\", \"day\", \"class_5_min\"]\n",
    "\n",
    "# Initialize an empty list to hold all records for class_5_min\n",
    "class_5_min_records = []\n",
    "\n",
    "# Process each UL user ID\n",
    "for user_id in user_ids_SA:\n",
    "    file_name = f\"daily_activity_{user_id}.json\"\n",
    "    file_path = os.path.join(directory_SA, file_name)\n",
    "    \n",
    "    # Try to open and process each file\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract the class_5_min data and day (date) for each entry\n",
    "        for entry in data.get(\"data\", []):\n",
    "            day = entry.get(\"day\", None)  # Date in the format YYYY-MM-DD\n",
    "            class_5_min = entry.get(\"class_5_min\", None)  # Five-minute interval activity classifications\n",
    "            \n",
    "            if day and class_5_min:\n",
    "                # Create a record for each 5-minute interval\n",
    "                for activity_class in class_5_min:\n",
    "                    # Append each interval as a record\n",
    "                    record = {\n",
    "                        \"user_id\": user_id,\n",
    "                        \"day\": day,\n",
    "                        \"class_5_min\": int(activity_class) if activity_class.isdigit() else None\n",
    "                    }\n",
    "                    class_5_min_records.append(record)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the records and save as a CSV file\n",
    "class_5_min_df = pd.DataFrame(class_5_min_records, columns=columns)\n",
    "class_5_min_csv_path = os.path.join(directory_SA, \"activity_intraday_SA.csv\")\n",
    "class_5_min_df.to_csv(class_5_min_csv_path, index=False)\n",
    "print(f\"Class 5-minute activity data saved: {class_5_min_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MET intraday data combined and saved: /Users/katherinetian/Downloads/Daily Activity/SA/met_intraday_activity_SA.csv\n"
     ]
    }
   ],
   "source": [
    "# Columns for the met_intraday table\n",
    "met_intraday_columns = [\"user_id\", \"datetime\", \"met_intraday\"]\n",
    "\n",
    "# Initialize an empty list to hold all records for met_intraday\n",
    "met_intraday_records = []\n",
    "\n",
    "# Process each UL user ID\n",
    "for user_id in user_ids_SA:\n",
    "    file_name = f\"daily_activity_{user_id}.json\"\n",
    "    file_path = os.path.join(directory_SA, file_name)\n",
    "    \n",
    "    # Try to open and process each file\n",
    "    try:\n",
    "        with open(file_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Extract MET data for each entry\n",
    "        for entry in data.get(\"data\", []):\n",
    "            met_data = entry.get(\"met\", {})  # MET data dictionary\n",
    "            \n",
    "            # Extract relevant fields from MET data\n",
    "            interval = met_data.get(\"interval\", None)  # Time interval in seconds between each MET measurement\n",
    "            start_timestamp = met_data.get(\"timestamp\", None)  # Start timestamp for the MET data sequence\n",
    "            items = met_data.get(\"items\", [])  # Array of MET values\n",
    "            \n",
    "            # Check if required MET data is present\n",
    "            if start_timestamp and interval and items:\n",
    "                # Convert the start timestamp to a datetime object\n",
    "                try:\n",
    "                    start_datetime = pd.to_datetime(start_timestamp)\n",
    "                    \n",
    "                    # Create a record for each MET value\n",
    "                    for i, met_value in enumerate(items):\n",
    "                        # Calculate the timestamp for each measurement based on the interval\n",
    "                        datetime_val = start_datetime + pd.Timedelta(seconds=interval * i)\n",
    "                        \n",
    "                        # Append the record to met_intraday_records\n",
    "                        record = {\n",
    "                            \"user_id\": user_id,\n",
    "                            \"datetime\": datetime_val,\n",
    "                            \"met_intraday\": met_value\n",
    "                        }\n",
    "                        met_intraday_records.append(record)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing MET timestamp for participant {user_id}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the met_intraday records\n",
    "met_intraday_df = pd.DataFrame(met_intraday_records, columns=met_intraday_columns)\n",
    "\n",
    "# Save to a CSV file\n",
    "met_intraday_csv_path = os.path.join(directory_SA, \"met_intraday_activity_SA.csv\")\n",
    "met_intraday_df.to_csv(met_intraday_csv_path, index=False)\n",
    "print(f\"MET intraday data combined and saved: {met_intraday_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA and UL data combined and saved: /Users/katherinetian/Downloads/Daily Activity/combined_daily_activity.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths to the SA and UL CSV files\n",
    "sa_file_path = '/Users/katherinetian/Downloads/Daily Activity/SA/daily_activity_SA.csv'\n",
    "ul_file_path = '/Users/katherinetian/Downloads/Daily Activity/UL/daily_activity_UL.csv'\n",
    "\n",
    "# Read each CSV file into a DataFrame\n",
    "sa_df = pd.read_csv(sa_file_path)\n",
    "ul_df = pd.read_csv(ul_file_path)\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "combined_df = pd.concat([sa_df, ul_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_csv_path = '/Users/katherinetian/Downloads/Daily Activity/combined_daily_activity.csv'\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"SA and UL data combined and saved: {combined_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA and UL data combined and saved: /Users/katherinetian/Downloads/Daily Activity/combined_activity_intraday.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths to the SA and UL CSV files\n",
    "sa_file_path = '/Users/katherinetian/Downloads/Daily Activity/SA/activity_intraday_SA.csv'\n",
    "ul_file_path = '/Users/katherinetian/Downloads/Daily Activity/UL/activity_intraday_UL.csv'\n",
    "\n",
    "# Read each CSV file into a DataFrame\n",
    "sa_df = pd.read_csv(sa_file_path)\n",
    "ul_df = pd.read_csv(ul_file_path)\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "combined_df = pd.concat([sa_df, ul_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_csv_path = '/Users/katherinetian/Downloads/Daily Activity/combined_activity_intraday.csv'\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"SA and UL data combined and saved: {combined_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA and UL data combined and saved: /Users/katherinetian/Downloads/Daily Activity/met_intraday_activity.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths to the SA and UL CSV files\n",
    "sa_file_path = '/Users/katherinetian/Downloads/Daily Activity/SA/met_intraday_activity_SA.csv'\n",
    "ul_file_path = '/Users/katherinetian/Downloads/Daily Activity/UL/met_intraday_activity_UL.csv'\n",
    "\n",
    "# Read each CSV file into a DataFrame\n",
    "sa_df = pd.read_csv(sa_file_path)\n",
    "ul_df = pd.read_csv(ul_file_path)\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "combined_df = pd.concat([sa_df, ul_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_csv_path = '/Users/katherinetian/Downloads/Daily Activity/met_intraday_activity.csv'\n",
    "combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "print(f\"SA and UL data combined and saved: {combined_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
